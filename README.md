# GCP Exam Prep Suite

This project contains three tools designed to help you prepare for Google Cloud Professional exams:
1.  **Scraper**: A robust Selenium-based tool to extract exam questions, options, and answers from *examprepper.co*, capable of bypassing WAF (Vercel/Cloudflare) checks.
2.  **Quiz App**: A terminal-based (TUI) simulation engine that lets you practice using the scraped data with a timer and scoring.
3.  **Converter**: A utility to transform the raw JSON data into a clean, readable Markdown document for offline study.

---

## üìã Prerequisites

*   **Python 3.12+**
*   **Google Chrome** (Latest stable version installed on your OS)
*   **uv** (Python package manager)

## üöÄ Installation

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd gcp-exam-scraper
    ```

2.  **Install dependencies:**
    This project uses `uv` for dependency management.
    ```bash
    uv sync
    # OR
    uv pip install -e .
    ```

---

## ‚öôÔ∏è Configuration

Create a `.env` file in the root directory to configure all applications.

### `.env` Reference

```ini
# --- SCRAPER SETTINGS ---
EXAM_NAME="GCP Professional Cloud Architect"
# The starting URL for the exam
START_URL="https://www.examprepper.co/exam/5/1"

# Output format: json (default), csv, or yaml
OUTPUT_FORMAT="json"
# The file where scraper saves data (and Converter reads from)
OUTPUT_FILE="exam_results.json"

# Browser visibility: true = background, false = visible window
# RECOMMENDED: Set to false to bypass CAPTCHAs manually
HEADLESS=false

# Optional: Scrape only a specific subset of questions
# Comment these out to scrape everything found
QUESTION_RANGE_START=5
QUESTION_RANGE_END=40

# --- QUIZ APP SETTINGS ---
# Path to the file generated by the scraper (usually same as OUTPUT_FILE)
EXAM_QUESTIONS_FILE="exam_results.json"

# Number of questions per quiz session
EXAM_MAX_QUESTIONS=10

# Time limit in minutes
EXAM_TIMER_MINUTES=15
```

---

## üï∑Ô∏è Part 1: The Scraper

The scraper uses `undetected-chromedriver` to navigate the exam site. It handles pagination, detecting correct answers (green borders/CSS classes), and saving data incrementally.

### Usage

Run the scraper module:

```bash
uv run -m scraper.main
```

### Features & Behavior
1.  **Manual Intervention Mode:**
    *   If the scraper detects a **Login Wall**, **CAPTCHA**, or **Cloudflare/Vercel security check**, it will **PAUSE** execution.
    *   It will beep (on supported terminals) and ask you to interact with the Chrome window.
    *   Once you have logged in or solved the CAPTCHA, press `ENTER` in the terminal to resume.
2.  **Smart Range:** If you set `QUESTION_RANGE_START` and `_END` in `.env`, the scraper will skip pages until it finds the specific questions you want.
3.  **Backups:** Before starting a new session, it automatically creates a timestamped backup of your existing `exam_results.json` (e.g., `backup_20251224_1200_exam_results.json`).
4.  **Incremental Save:** Data is saved after every page to prevent data loss.

---

## üìù Part 2: The Quiz App

The Quiz App loads the JSON data generated by the scraper and presents it in a beautiful CLI interface.

### Usage

Run the quiz module:

```bash
uv run -m quiz_app.main
```

### Controls
*   **Single Choice:** Use `‚Üë` / `‚Üì` arrows to highlight, `ENTER` to select.
*   **Multiple Choice:** Use `‚Üë` / `‚Üì` to navigate, `SPACE` to toggle options, `ENTER` to confirm.
*   **Cancel:** Press `Ctrl+C` to exit early.

### Logs
The quiz interface is kept clean. Detailed logs of your session (questions asked, answers selected, pass/fail status) are written to:
`logs/quiz_session.log`

---

## üìÑ Part 3: The Converter

The Converter allows you to export your scraped JSON database into a formatted Markdown file. This is perfect for importing into knowledge bases (Obsidian, Notion) or reading on GitHub.

### Usage

Run the converter module:

```bash
uv run -m converter.main
```

### Output
The script generates **`exam_export.md`** in the project root.
*   **Format**: Clean Markdown with headers for Question IDs.
*   **Options**: Checkboxes allow you to simulate answering on paper.
*   **Answers**: Correct answers are highlighted with **bold text** and a ‚úÖ icon.

---

## üõ†Ô∏è Troubleshooting

**1. `SessionNotCreatedException` / Chrome version mismatch**
*   The scraper is set to auto-detect your Chrome version.
*   Ensure you have a standard Google Chrome installed: `google-chrome --version` (Linux) or check "About Chrome" (Windows/Mac).
*   If it fails, try updating your Chrome browser.

**2. Scraper immediately crashes or times out**
*   Set `HEADLESS=false` in your `.env`.
*   WAFs (Web Application Firewalls) often block headless browsers immediately. Running visually usually solves this.

**3. "No module named scraper"**
*   Ensure you are running the command using `uv run -m scraper.main` from the root folder.
*   Do not run `python src/scraper/main.py` directly unless you manually set `PYTHONPATH`.