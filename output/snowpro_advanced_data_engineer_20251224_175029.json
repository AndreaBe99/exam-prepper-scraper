[
    {
        "id": "Question 1",
        "text": "A Data Engineer is investigating a query that is taking a long time to return. The Query Profile shows the following:\nWhat step should the Engineer take to increase the query performance?",
        "options": {
            "A": "Add additional virtual warehouses.",
            "B": "Increase the size of the virtual warehouse.",
            "C": "Rewrite the query using Common Table Expressions (CTEs).",
            "D": "Change the order of the joins and start with smaller tables first."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 2",
        "text": "Which methods can be used to create a DataFrame object in Snowpark? (Choose three.)",
        "options": {
            "A": "session.jdbc_connection()",
            "B": "session.read.json()",
            "C": "session.table()",
            "D": "DataFrame.write()",
            "E": "session.builder()",
            "F": "session.sql()"
        },
        "correct_answers": [
            "B",
            "C",
            "F"
        ]
    },
    {
        "id": "Question 3",
        "text": "A Data Engineer has created a scalar User-Defined Function (UDF) that contains a SELECT statement based on a projection constraint column. The Engineer notices that the UDF behaves differently depending on how it is being used.\nWhat are the expected behaviors of the UDF for the named context? (Choose two.)",
        "options": {
            "A": "Scalar SQL UDF - Snowflake allows the query to execute and returns NULL.",
            "B": "Scalar SQL UDF - Snowflake blocks the query.",
            "C": "Logging & Event Tables - Snowflake allows the UDF to execute and captures log and event details.",
            "D": "Logging & Event Tables - Snowflake allows the UDF to execute but does not capture log and event details.",
            "E": "Logging & Event Tables - Snowflake blocks the UDF but allows the statement calling the UDF to run to support logging requirements."
        },
        "correct_answers": [
            "B",
            "D"
        ]
    },
    {
        "id": "Question 4",
        "text": "A new CUSTOMER table is created by a data pipeline in a Snowflake schema where MANAGED ACCESS is enabled.\nWhich roles can grant access to the CUSTOMER table? (Choose three.)",
        "options": {
            "A": "The role that owns the schema",
            "B": "The role that owns the database",
            "C": "The role that owns the CUSTOMER table",
            "D": "The SYSADMIN role",
            "E": "The SECURITYADMIN role",
            "F": "The USERADMIN role with the MANAGE GRANTS privilege"
        },
        "correct_answers": [
            "A",
            "E",
            "F"
        ]
    },
    {
        "id": "Question 5",
        "text": "What is the purpose of the BUILD_STAGE_FILE_URL function in Snowflake?",
        "options": {
            "A": "It generates an encrypted URL for accessing a file in a stage.",
            "B": "It generates a staged URL for accessing a file in a stage.",
            "C": "It generates a permanent URL for accessing files in a stage.",
            "D": "It generates a temporary URL for accessing a file in a stage."
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 6",
        "text": "The JSON below is stored in a VARIANT column named V in a table named jCustRaw:\nWhich query will return one row per team member (stored in the teamMembers array) along with all of the attributes of each team member?",
        "options": {
            "A": "",
            "B": "",
            "C": "",
            "D": ""
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 7",
        "text": "A company has an extensive script in Scala that transforms data by leveraging DataFrames. A Data Engineer needs to move these transformations to Snowpark.\nWhat characteristics of data transformations in Snowpark should be considered to meet this requirement? (Choose two.)",
        "options": {
            "A": "It is possible to join multiple tables using DataFrames.",
            "B": "Snowpark operations are executed lazily on the server.",
            "C": "User-Defined Functions (UDFs) are not pushed down to Snowflake.",
            "D": "Snowpark requires a separate cluster outside of Snowflake for computations.",
            "E": "Columns in different DataFrames with the same name should be referred to with squared brackets."
        },
        "correct_answers": [
            "A",
            "B"
        ]
    },
    {
        "id": "Question 8",
        "text": "The following is returned from SYSTEM$CLUSTERING_INFORMATION() for a table named ORDERS with a DATE column named O_ORDERDATE:\nWhat does the total_constant_partition_count value indicate about this table?",
        "options": {
            "A": "The table is clustered very well on O_ORDERDATE, as there are 493 micro-partitions that could not be significantly improved by reclustering.",
            "B": "The table is not clustered well on O_ORDERDATE, as there are 493 micro-partitions where the range of values in that column overlap with every other micro-partition in the table.",
            "C": "The data in O_ORDERDATE does not change very often, as there are 493 micro-partitions containing rows where that column has not been modified since the row was created.",
            "D": "The data in O_ORDERDATE has a very low cardinality, as there are 493 micro-partitions where there is only a single distinct value in that column for all rows in the micro-partition."
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 9",
        "text": "A company is building a dashboard for thousands of Analysts. The dashboard presents the results of a few summary queries on tables that are regularly updated. The query conditions vary by topic according to what data each Analyst needs. Responsiveness of the dashboard queries is a top priority, and the data cache should be preserved.\nHow should the Data Engineer configure the compute resources to support this dashboard?",
        "options": {
            "A": "Assign queries to a multi-cluster virtual warehouse with economy auto-scaling. Allow the system to automatically start and stop clusters according to demand.",
            "B": "Assign all queries to a multi-cluster virtual warehouse set to maximized mode. Monitor to determine the smallest suitable number of clusters.",
            "C": "Create a virtual warehouse for every 250 Analysts. Monitor to determine how many of these virtual warehouses are being utilized at capacity.",
            "D": "Create a size XL virtual warehouse to support all the dashboard queries. Monitor query runtimes to determine whether the virtual warehouse should be resized."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 10",
        "text": "A Data Engineer has developed a dashboard that will issue the same SQL select clause to Snowflake every 12 hours.\nHow long will Snowflake use the persisted query results from the result cache, provided that the underlying data has not changed?",
        "options": {
            "A": "12 hours",
            "B": "24 hours",
            "C": "14 days",
            "D": "31 days"
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 11",
        "text": "A Data Engineer ran a stored procedure containing various transactions. During the execution, the session abruptly disconnected, preventing one transaction from committing or rolling back. The transaction was left in a detached state and created a lock on resources.\nWhat step must the Engineer take to immediately run a new transaction?",
        "options": {
            "A": "Call the system function SYSTEM$ABORT_TRANSACTION.",
            "B": "Call the system function SYSTEM$CANCEL_TRANSACTION.",
            "C": "Set the LOCK_TIMEOUT to FALSE in the stored procedure.",
            "D": "Set the TRANSACTION_ABORT_ON_ERROR to TRUE in the stored procedure."
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 12",
        "text": "A database contains a table and a stored procedure defined as:\nThe log_table is initially empty and a Data Engineer issues the following command:\nCALL insert_log(NULL::VARCHAR);\nNo other operations are affecting the log_table.\nWhat will be the outcome of the procedure call?",
        "options": {
            "A": "The log_table contains zero records and the stored procedure returned 1 as a return value.",
            "B": "The log_table contains one record and the stored procedure returned 1 as a return value.",
            "C": "The log_table contains one record and the stored procedure returned NULL as a return value.",
            "D": "The log_table contains zero records and the stored procedure returned NULL as a return value."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 13",
        "text": "How can the following relational data be transformed into semi-structured data using the LEAST amount of operational overhead?",
        "options": {
            "A": "Use the TO_JSON function.",
            "B": "Use the PARSE_JSON function to produce a VARIANT value.",
            "C": "Use the OBJECT_CONSTRUCT function to return a Snowflake object.",
            "D": "Use the TO_VARIANT function to convert each of the relational columns to VARIANT."
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 14",
        "text": "When would a Data Engineer use TABLE with the FLATTEN function instead of the LATERAL FLATTEN combination?",
        "options": {
            "A": "When TABLE with FLATTEN requires another source in the FROM clause to refer to.",
            "B": "When TABLE with FLATTEN requires no additional source in the FROM clause to refer to.",
            "C": "When the LATERAL FLATTEN combination requires no other source in the FROM clause to refer to.",
            "D": "When TABLE with FLATTEN is acting like a sub-query executed for each returned row."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 15",
        "text": "Which output is provided by both the SYSTEM$CLUSTERING_DEPTH function and the SYSTEM$CLUSTERING_INFORMATION function?",
        "options": {
            "A": "average_depth",
            "B": "notes",
            "C": "average_overlaps",
            "D": "total_partition_count"
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 16",
        "text": "A Data Engineer needs to ingest invoice data in PDF format into Snowflake so that the data can be queried and used in a forecasting solution.\nWhat is the recommended way to ingest this data?",
        "options": {
            "A": "Use Snowpipe to ingest the files that land in an external stage into a Snowflake table.",
            "B": "Use a COPY INTO command to ingest the PDF files in an external stage into a Snowflake table with a VARIANT column.",
            "C": "Create an external table on the PDF files that are stored in a stage and parse the data into structured data.",
            "D": "Create a Java User-Defined Function (UDF) that leverages Java-based PDF parser libraries to parse PDF data into structured data."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 17",
        "text": "A table is loaded using Snowpipe and truncated afterwards. Later, a Data Engineer finds that the table needs to be reloaded, but the metadata of the pipe will not allow the same files to be loaded again.\nHow can this issue be solved using the LEAST amount of operational overhead?",
        "options": {
            "A": "Wait until the metadata expires and then reload the file using Snowpipe.",
            "B": "Modify the file by adding a blank row to the bottom and re-stage the file.",
            "C": "Set the FORCE=TRUE option in the Snowpipe COPY INTO command.",
            "D": "Recreate the pipe by using the CREATE OR REPLACE PIPE command."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 18",
        "text": "A stream called TRANSACTIONS_STM is created on top of a TRANSACTIONS table in a continuous pipeline running in Snowflake. After a couple of months, the TRANSACTIONS table is renamed TRANSACTIONS_RAW to comply with new naming standards.\nWhat will happen to the TRANSACTIONS_STM object?",
        "options": {
            "A": "TRANSACTIONS_STM will keep working as expected.",
            "B": "TRANSACTIONS_STM will be stale and will need to be re-created.",
            "C": "TRANSACTIONS_STM will be automatically renamed TRANSACTIONS_RAW_STM.",
            "D": "Reading from the TRANSACTIONS_STM stream will succeed for some time after the expected STALE_TIME."
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 19",
        "text": "A Data Engineer is evaluating the performance of a query in a development environment.\nBased on the Query Profile, what are some performance tuning options the Engineer can use? (Choose two.)",
        "options": {
            "A": "Add a LIMIT to the ORDER BY if possible",
            "B": "Use a multi-cluster virtual warehouse with the scaling policy set to standard",
            "C": "Move the query to a larger virtual warehouse",
            "D": "Create indexes to ensure sorted access to data",
            "E": "Increase the MAX_CLUSTER_COUNT"
        },
        "correct_answers": [
            "A",
            "C"
        ]
    },
    {
        "id": "Question 20",
        "text": "Which methods will trigger an action that will evaluate a DataFrame? (Choose two.)",
        "options": {
            "A": "DataFrame.random_split()",
            "B": "DataFrame.collect()",
            "C": "DataFrame.select()",
            "D": "DataFrame.col()",
            "E": "DataFrame.show()"
        },
        "correct_answers": [
            "B",
            "E"
        ]
    },
    {
        "id": "Question 21",
        "text": "Which Snowflake objects does the Snowflake Kafka connector use? (Choose three.)",
        "options": {
            "A": "Pipe",
            "B": "Serverless task",
            "C": "Internal user stage",
            "D": "Internal table stage",
            "E": "Internal named stage",
            "F": "Storage integration"
        },
        "correct_answers": [
            "A",
            "D",
            "E"
        ]
    },
    {
        "id": "Question 22",
        "text": "A Data Engineer has created table t1 with one column c1 with datatype VARIANT: create or replace table t1 (c1 variant);\nThe Engineer has loaded the following JSON data set, which has information about 4 laptop models, into the table.\nThe Engineer now wants to query that data set so that results are shown as normal structured data. The result should be 4 rows and 4 columns, without the double quotes surrounding the data elements in the JSON data.\nThe result should be similar to the use case where the data was selected from a normal relational table t2, where t2 has string data type columns model_id, model, manufacturer, and model_name, and is queried with the SQL clause select * from t2;\nWhich select command will produce the correct results?",
        "options": {
            "A": "",
            "B": "",
            "C": "",
            "D": ""
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 23",
        "text": "What is a characteristic of the use of external tokenization?",
        "options": {
            "A": "Secure data sharing can be used with external tokenization.",
            "B": "External tokenization cannot be used with database replication.",
            "C": "Pre-loading of unmasked data is supported with external tokenization.",
            "D": "External tokenization allows the preservation of analytical values after de-identification."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 24",
        "text": "A Data Engineer executes a complex query and wants to make use of Snowflake’s query results caching capabilities to reuse the results.\nWhich conditions must be met? (Choose three.)",
        "options": {
            "A": "The results must be reused within 72 hours.",
            "B": "The query must be executed using the same virtual warehouse.",
            "C": "The USED_CACHED_RESULT parameter must be included in the query.",
            "D": "The table structure contributing to the query result cannot have changed.",
            "E": "The new query must have the same syntax as the previously executed query.",
            "F": "The micro-partitions cannot have changed due to changes to other data in the table."
        },
        "correct_answers": [
            "D",
            "E",
            "F"
        ]
    },
    {
        "id": "Question 25",
        "text": "A Data Engineer is implementing a near real-time ingestion pipeline to load data into Snowflake using the Snowflake Kafka connector. There will be three Kafka topics created.\nWhich Snowflake objects are created automatically when the Kafka connector starts? (Choose three.)",
        "options": {
            "A": "Tables",
            "B": "Tasks",
            "C": "Pipes",
            "D": "Internal stages",
            "E": "External stages",
            "F": "Materialized views"
        },
        "correct_answers": [
            "A",
            "C",
            "D"
        ]
    },
    {
        "id": "Question 26",
        "text": "The following chart represents the performance of a virtual warehouse over time:\nA Data Engineer notices that the warehouse is queueing queries. The warehouse is size X-Small, the minimum and maximum cluster counts are set to 1, the scaling policy is set to standard, and auto-suspend is set to 10 minutes.\nHow can the performance be improved?",
        "options": {
            "A": "Change the cluster settings.",
            "B": "Increase the size of the warehouse.",
            "C": "Change the scaling policy to economy.",
            "D": "Change auto-suspend to a longer time frame."
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 27",
        "text": "A secure function returns data coming through an inbound share.\nWhat will happen if a Data Engineer tries to assign USAGE privileges on this function to an outbound share?",
        "options": {
            "A": "An error will be returned because the Engineer cannot share data that has already been shared.",
            "B": "An error will be returned because only views and secure stored procedures can be shared.",
            "C": "An error will be returned because only secure functions can be shared with inbound shares.",
            "D": "The Engineer will be able to share the secure function with other accounts."
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 28",
        "text": "Which functions will compute a 'fingerprint' over an entire table, query result, or window to quickly detect changes to table contents or query results? (Choose two.)",
        "options": {
            "A": "HASH(*)",
            "B": "HASH_AGG(*)",
            "C": "HASH_AGG(<expr>, <expr>)",
            "D": "HASH_AGG_COMPARE(*)",
            "E": "HASH_COMPARE(*)"
        },
        "correct_answers": [
            "B",
            "C"
        ]
    },
    {
        "id": "Question 29",
        "text": "Which stages support external tables?",
        "options": {
            "A": "Internal stages only; within a single Snowflake account",
            "B": "Internal stages only; from any Snowflake account in the organization",
            "C": "External stages only; from any region, and any cloud provider",
            "D": "External stages only; only on the same region and cloud provider as the Snowflake account"
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 30",
        "text": "A Data Engineer wants to check the status of a pipe named my_pipe. The pipe is inside a database named test and a schema named Extract (case-sensitive).\nWhich query will provide the status of the pipe?",
        "options": {
            "A": "SELECT SYSTEM$PIPE_STATUS(\"test.'extract'.my_pipe\");",
            "B": "SELECT SYSTEM$PIPE_STATUS('test.\"Extract\".my_pipe');",
            "C": "SELECT * FROM SYSTEM$PIPE_STATUS('test.\"Extract\".my_pipe');",
            "D": "SELECT * FROM SYSTEM$PIPE_STATUS(\"test.'extract'.my_pipe\");"
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 31",
        "text": "Company A and Company B both have Snowflake accounts. Company A's account is hosted on a different cloud provider and region than Company B's account. Companies A and B are not in the same Snowflake organization.\nHow can Company A share data with Company B? (Choose two.)",
        "options": {
            "A": "Create a share within Company A's account and add Company B's account as a recipient of that share.",
            "B": "Create a share within Company A's account, and create a reader account that is a recipient of the share. Grant Company B access to the reader account.",
            "C": "Use database replication to replicate Company A's data into Company B's account. Create a share within Company B's account and grant users within Company B's account access to the share.",
            "D": "Create a new account within Company A's organization in the same cloud provider and region as Company B's account. Use database replication to replicate Company A's data to the new account. Create a share within the new account, and add Company B's account as a recipient of that share.",
            "E": "Create a separate database within Company A's account to contain only those data sets they wish to share with Company B. Create a share within Company A's account and add all the objects within this separate database to the share. Add Company B's account as a recipient of the share."
        },
        "correct_answers": [
            "A",
            "D"
        ]
    },
    {
        "id": "Question 32",
        "text": "A Data Engineer is trying to load the following rows from a CSV file into a table in Snowflake with the following structure:\nThe engineer is using the following COPY INTO statement:\nHowever, the following error is received:\nNumber of columns in file (6) does not match that of the corresponding table (3), use file format option error_on_column_count_mismatch=false to ignore this error File 'address.csv.gz', line 3, character 1 Row 1 starts at line 2, column \"STGCUSTOMER\"[6] If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option.\nWhich file format option should be used to resolve the error and successfully load all the data into the table?",
        "options": {
            "A": "ESCAPE_UNENCLOSED FIELD = '\\\\'",
            "B": "ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE",
            "C": "FIELD_DELIMITER = ','",
            "D": "FIELD_OPTIONALLY_ENCLOSED_BY = '\"'"
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 33",
        "text": "A Data Engineer is working on a continuous data pipeline which receives data from Amazon Kinesis Firehose and loads the data into a staging table which will later be used in the data transformation process. The average file size is 300-500 MB.\nThe Engineer needs to ensure that Snowpipe is performant while minimizing costs.\nHow can this be achieved?",
        "options": {
            "A": "Increase the size of the virtual warehouse used by Snowpipe.",
            "B": "Split the files before loading them and set the SIZE_LIMIT option to 250 MB.",
            "C": "Change the file compression size and increase the frequency of the Snowpipe loads.",
            "D": "Decrease the buffer size to trigger delivery of files sized between 100 to 250 MB in Kinesis Firehose."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 34",
        "text": "Within a Snowflake account. permissions have been defined with custom roles and role hierarchies.\nTo set up column-level masking using a role in the hierarchy of the current user, what command would be used?",
        "options": {
            "A": "CURRENT_ROLE",
            "B": "INVOKER_ROLE",
            "C": "IS_ROLE_IN_SESSION",
            "D": "IS_GRANTED_TO_INVOKER_ROLE"
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 35",
        "text": "A Data Engineer needs to load JSON output from some software into Snowflake using Snowpipe.\nWhich recommendations apply to this scenario? (Choose three.)",
        "options": {
            "A": "Load large files (1 GB or larger).",
            "B": "Ensure that data files are 100-250 MB (or larger) in size, compressed.",
            "C": "Load a single huge array containing multiple records into a single table row.",
            "D": "Verify each value of each unique element stores a single native data type (string or number).",
            "E": "Extract semi-structured data elements containing null values into relational columns before loading.",
            "F": "Create data files that are less than 100 MB and stage them in cloud storage at a sequence greater than once each minute."
        },
        "correct_answers": [
            "B",
            "D",
            "E"
        ]
    },
    {
        "id": "Question 36",
        "text": "Assuming a Data Engineer has all appropriate privileges and context, which statements would be used to assess whether the User-Defined Function (UDF), MYDATABASE.SALES.REVENUE_BY_REGION, exists and is secure? (Choose two.)",
        "options": {
            "A": "SHOW USER FUNCTIONS LIKE 'REVENUE_BY_REGION' IN SCHEMA SALES;",
            "B": "SELECT IS_SECURE FROM SNOWFLAKE.INFORMATION_SCHEMA.FUNCTIONS WHERE FUNCTION_SCHEMA = 'SALES' AND FUNCTION_NAME = 'REVENUE_BY_REGION';",
            "C": "SELECT IS_SECURE FROM INFORMATION_SCHEMA.FUNCTIONS WHERE FUNCTION_SCHEMA = 'SALES' AND FUNCTION_NAME = 'REVENUE_BY_REGION';",
            "D": "SHOW EXTERNAL FUNCTIONS LIKE 'REVENUE_BY_REGION' IN SCHEMA SALES;",
            "E": "SHOW SECURE FUNCTIONS LIKE 'REVENUE_BY_REGION' IN SCHEMA SALES;"
        },
        "correct_answers": [
            "A",
            "C"
        ]
    },
    {
        "id": "Question 37",
        "text": "A Data Engineer has written a stored procedure that will run with caller's rights. The Engineer has granted ROLEA the right to use this stored procedure.\nWhat is a characteristic of the stored procedure being called using ROLEA?",
        "options": {
            "A": "The stored procedure must run with caller's rights; it cannot be converted later to run with owner's rights.",
            "B": "If the stored procedure accesses an object that ROLEA does not have access to, the stored procedure will fail.",
            "C": "The stored procedure will run in the context (database and schema) where the owner created the stored procedure.",
            "D": "ROLEA will not be able to see the source code for the stored procedure, even though the role has usage privileges on the stored procedure."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 38",
        "text": "What is a characteristic of the use of binding variables in JavaScript stored procedures in Snowflake?",
        "options": {
            "A": "All types of JavaScript variables can be bound.",
            "B": "All Snowflake first-class objects can be bound.",
            "C": "Only JavaScript variables of type number, string, and SfDate can be bound.",
            "D": "Users are restricted from binding JavaScript variables because they create SQL injection attack vulnerabilities."
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 39",
        "text": "Which use case would be BEST suited for the search optimization service?",
        "options": {
            "A": "Analysts who need to perform aggregates over high-cardinality columns.",
            "B": "Business users who need fast response times using highly selective filters.",
            "C": "Data Scientists who seek specific JOIN statements with large volumes of data.",
            "D": "Data Engineers who create clustered tables with frequent reads against clustering keys."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 40",
        "text": "What is a characteristic of the operations of streams in Snowflake?",
        "options": {
            "A": "Whenever a stream is queried, the offset is automatically advanced.",
            "B": "When a stream is used to update a target table, the offset is advanced to the current time.",
            "C": "Querying a stream returns all change records and table rows from the current offset to the current time.",
            "D": "Each committed and uncommitted transaction on the source table automatically puts a change record in the stream."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 41",
        "text": "At what isolation level are Snowflake streams?",
        "options": {
            "A": "Snapshot",
            "B": "Repeatable read",
            "C": "Read committed",
            "D": "Read uncommitted"
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 42",
        "text": "What kind of Snowflake integration is required when defining an external function in Snowflake?",
        "options": {
            "A": "API integration",
            "B": "HTTP integration",
            "C": "Notification integration",
            "D": "Security integration"
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 43",
        "text": "A Data Engineer is writing a Python script using the Snowflake Connector for Python. The Engineer will use the snowflake.connector.connect function to connect to Snowflake.\nThe requirements are:\nRaise an exception if the specified database, schema, or warehouse does not exist\nImprove download performance -\nWhich parameters of the connect function should be used? (Choose two.)",
        "options": {
            "A": "authenticator",
            "B": "arrow_number_to_decimal",
            "C": "client_prefetch_threads",
            "D": "client_session_keep_alive",
            "E": "validate_default_parameters"
        },
        "correct_answers": [
            "C",
            "E"
        ]
    },
    {
        "id": "Question 44",
        "text": "A Data Engineer wants to centralize grant management to maximize security. A user needs OWNERSHIP on a table in a new schema. However, this user should not have the ability to make grant decisions.\nWhat is the correct way to do this?",
        "options": {
            "A": "Grant OWNERSHIP to the user on the table.",
            "B": "Revoke grant decisions from the user on the table.",
            "C": "Revoke grant decisions from the user on the schema.",
            "D": "Add the WITH MANAGED ACCESS parameter on the schema."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 45",
        "text": "A CSV file, around 1 TB in size, is generated daily on an on-premise server. A corresponding table, internal stage, and file format have already been created in Snowflake to facilitate the data loading process.\nHow can the process of bringing the CSV file into Snowflake be automated using the LEAST amount of operational overhead?",
        "options": {
            "A": "Create a task in Snowflake that executes once a day and runs a COPY INTO statement that references the internal stage. The internal stage will read the files directly from the on-premise server and copy the newest file into the table from the on-premise server to the Snowflake table.",
            "B": "On the on-premise server, schedule a SQL file to run using SnowSQL that executes a PUT to push a specific file to the internal stage. Create a task that executes once a day in Snowflake and runs a COPY INTO statement that references the internal stage. Schedule the task to start after the file lands in the internal stage.",
            "C": "On the on-premise server, schedule a SQL file to run using SnowSQL that executes a PUT to push a specific file to the internal stage. Create a pipe that runs a COPY INTO statement that references the internal stage. Snowpipe auto-ingest will automatically load the file from the internal stage when the new file lands in the internal stage.",
            "D": "On the on-premise server, schedule a Python file that uses the Snowpark Python library. The Python script will read the CSV data into a DataFrame and generate an INSERT INTO statement that will directly load into the table. The script will bypass the need to move a file into an internal stage."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 46",
        "text": "Given the table SALES which has a clustering key of column CLOSED_DATE, which table function will return the average clustering depth for the SALES_REPRESENTATIVE column for the North American region?",
        "options": {
            "A": "select system$clustering_information('Sales', 'sales_representative', 'region = ''North America''');",
            "B": "select system$clustering_depth('Sales', 'sales_representative', 'region = ''North America''');",
            "C": "select system$clustering_depth('Sales', 'sales_representative') where region = 'North America';",
            "D": "select system$clustering_information('Sales', 'sales_representative') where region = 'North America’;"
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 47",
        "text": "What are characteristics of Snowpark Python packages? (Choose three.)",
        "options": {
            "A": "Third-party packages can be registered as a dependency to the Snowpark session using the session.import() method.",
            "B": "Python packages can access any external endpoints.",
            "C": "Python packages can only be loaded in a local environment.",
            "D": "Third-party supported Python packages are locked down to prevent hitting.",
            "E": "The SQL command DESCRIBE FUNCTION will list the imported Python packages of the Python User-Defined Function (UDF).",
            "F": "Querying information_schema.packages will provide a list of supported Python packages and versions."
        },
        "correct_answers": [
            "A",
            "E",
            "F"
        ]
    },
    {
        "id": "Question 48",
        "text": "While running an external function, the following error message is received:\nError: Function received the wrong number of rows\nWhat is causing this to occur?",
        "options": {
            "A": "External functions do not support multiple rows.",
            "B": "Nested arrays are not supported in the JSON response.",
            "C": "The JSON returned by the remote service is not constructed correctly.",
            "D": "The return message did not produce the same number of rows that it received."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 49",
        "text": "A Data Engineer enables a result cache at the session level with the following command:\nALTER SESSION SET USE_CACHED_RESULT = TRUE;\nThe Engineer then runs the following SELECT query twice without delay:\nSELECT *\nFROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\nSAMPLE(10) SEED (99);\nThe underlying table does not change between executions.\nWhat are the results of both runs?",
        "options": {
            "A": "The first and second run returned the same results, because SAMPLE is deterministic.",
            "B": "The first and second run returned the same results, because the specific SEED value was provided.",
            "C": "The first and second run returned different results, because the query is evaluated each time it is run.",
            "D": "The first and second run returned different results, because the query uses * instead of an explicit column list."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 50",
        "text": "A company built a sales reporting system with Python, connecting to Snowflake using the Python Connector. Based on the user's selections, the system generates the SQL queries needed to fetch the data for the report. First it gets the customers that meet the given query parameters (on average 1000 customer records for each report run), and then it loops the customer records sequentially. Inside that loop it runs the generated SQL clause for the current customer to get the detailed data for that customer number from the sales data table.\nWhen the Data Engineer tested the individual SQL clauses, they were fast enough (1 second to get the customers, 0.5 second to get the sales data for one customer), but the total runtime of the report is too long.\nHow can this situation be improved?",
        "options": {
            "A": "Increase the size of the virtual warehouse.",
            "B": "Increase the number of maximum clusters of the virtual warehouse.",
            "C": "Define a clustering key for the sales data table.",
            "D": "Rewrite the report to eliminate the use of the loop construct."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 51",
        "text": "A company is using Snowpipe to bring in millions of rows every day of Change Data Capture (CDC) into a Snowflake staging table on a real-time basis. The CDC needs to get processed and combined with other data in Snowflake and land in a final table as part of the full data pipeline.\nHow can a Data Engineer MOST efficiently process the incoming CDC on an ongoing basis?",
        "options": {
            "A": "Create a stream on the staging table and schedule a task that transforms data from the stream, only when the stream has data.",
            "B": "Transform the data during the data load with Snowpipe by modifying the related COPY INTO statement to include transformation steps such as CASE statements and JOINS.",
            "C": "Schedule a task that dynamically retrieves the last time the task was run from information_schema.task_history and use that timestamp to process the delta of the new rows since the last time the task was run.",
            "D": "Use a CREATE OR REPLACE TABLE AS statement that references the staging table and includes all the transformation SQL. Use a task to run the full CREATE OR REPLACE TABLE AS statement on a scheduled basis."
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 52",
        "text": "A Data Engineer is building a pipeline to transform a 1 TB table by joining it with supplemental tables. The Engineer is applying filters and several aggregations leveraging Common Table Expressions (CTEs) using a size Medium virtual warehouse in a single query in Snowflake.\nAfter checking the Query Profile, what is the recommended approach to MAXIMIZE performance of this query if the Profile shows data spillage?",
        "options": {
            "A": "Enable clustering on the table.",
            "B": "Increase the warehouse size.",
            "C": "Rewrite the query to remove the CTEs.",
            "D": "Switch to a multi-cluster virtual warehouse."
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 53",
        "text": "Which system role is recommended for a custom role hierarchy to be ultimately assigned to?",
        "options": {
            "A": "ACCOUNTADMIN",
            "B": "SECURITYADMIN",
            "C": "SYSADMIN",
            "D": "USERADMIN"
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 54",
        "text": "Which callback function is required within a JavaScript User-Defined Function (UDF) for it to execute successfully?",
        "options": {
            "A": "initialize()",
            "B": "processRow()",
            "C": "handler()",
            "D": "finalize()"
        },
        "correct_answers": [
            "B"
        ]
    },
    {
        "id": "Question 55",
        "text": "Which Snowflake feature facilitates access to external API services such as geocoders, data transformation, machine learning models, and other custom code?",
        "options": {
            "A": "Security integration",
            "B": "External tables",
            "C": "External functions",
            "D": "Java User-Defined Functions (UDFs)"
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 56",
        "text": "A Data Engineer needs to know the details regarding the micro-partition layout for a table named Invoice using a built-in function.\nWhich query will provide this information?",
        "options": {
            "A": "SELECT SYSTEM$CLUSTERING_INFORMATION('Invoice');",
            "B": "SELECT $CLUSTERING_INFORMATION('Invoice');",
            "C": "CALL SYSTEM$CLUSTERING_INFORMATION('Invoice');",
            "D": "CALL $CLUSTERING_INFORMATION('Invoice');"
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 57",
        "text": "A large table with 200 columns contains two years of historical data. When queried, the table is filtered on a single day. Below is the Query Profile:\nUsing a size 2XL virtual warehouse, this query took over an hour to complete.\nWhat will improve the query performance the MOST?",
        "options": {
            "A": "Increase the size of the virtual warehouse.",
            "B": "Increase the number of clusters in the virtual warehouse.",
            "C": "Implement the search optimization service on the table.",
            "D": "Add a date column as a cluster key on the table."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 58",
        "text": "A Data Engineer would like to define a file structure for loading and unloading data.\nWhere can the file structure be defined? (Choose three.)",
        "options": {
            "A": "COPY command",
            "B": "MERGE command",
            "C": "FILE FORMAT object",
            "D": "PIPE object",
            "E": "STAGE object",
            "F": "INSERT command"
        },
        "correct_answers": [
            "A",
            "C",
            "D"
        ]
    },
    {
        "id": "Question 59",
        "text": "Assuming that the session parameter USE_CACHED_RESULT is set to false, what are characteristics of Snowflake virtual warehouses in terms of the use of Snowpark?",
        "options": {
            "A": "Creating a DataFrame from a table will start a virtual warehouse.",
            "B": "Creating a DataFrame from a staged file with the read() method will start a virtual warehouse.",
            "C": "Transforming a DataFrame with methods like replace() will start a virtual warehouse.",
            "D": "Calling a Snowpark stored procedure to query the database with session.call() will start a virtual warehouse."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 60",
        "text": "Database XYZ has the data_retention_time_in_days parameter set to 7 days and table XYZ.public.ABC has the data_retention_time_in_days set to 10 days.\nA Developer accidentally dropped the database containing this single table 8 days ago and just discovered the mistake.\nHow can the table be recovered?",
        "options": {
            "A": "undrop database xyz;",
            "B": "create table abc_restore as select * from xyz.public.abc at (offset => -60*60*24*8);",
            "C": "create table abc_restore clone xyz.public.abc at (offset => -3600*24*8);",
            "D": "Create a Snowflake Support case to restore the database and table from Fail-safe."
        },
        "correct_answers": [
            "D"
        ]
    },
    {
        "id": "Question 61",
        "text": "A Data Engineer is building a set of reporting tables to analyze consumer requests by region for each of the Data Exchange offerings annually, as well as click-through rates for each listing.\nWhich views are needed MINIMALLY as data sources?",
        "options": {
            "A": "SNOWFLAKE.DATA_SHARING_USAGE.LISTING_EVENTS_DAILY",
            "B": "SNOWFLAKE.DATA_SHARING_USAGE.LISTING_CONSUMPTION_DAILY",
            "C": "SNOWFLAKE.DATA_SHARING_USAGE.LISTING_TELEMETRY_DAILY",
            "D": "SNOWFLAKE.ACCOUNT_USAGE.DATA_TRANSFER_HISTORY"
        },
        "correct_answers": [
            "B",
            "C"
        ]
    },
    {
        "id": "Question 62",
        "text": "The following code is executed in a Snowflake environment with the default settings:\nWhat will be the result of the select statement?",
        "options": {
            "A": "SQL compilation error: Object 'CUSTOMER' does not exist or is not authorized.",
            "B": "John",
            "C": "1",
            "D": "1John"
        },
        "correct_answers": [
            "A"
        ]
    },
    {
        "id": "Question 63",
        "text": "A Data Engineer defines the following masking policy:\nThe policy must be applied to the full_name column in the customer table:\nWhich query will apply the masking policy on the full_name column?",
        "options": {
            "A": "ALTER TABLE customer MODIFY COLUMN full_name SET MASKING POLICY name policy;",
            "B": "ALTER TABLE customer MODIFY COLUMN full_name ADD MASKING POLICY name_policy;",
            "C": "ALTER TABLE customer MODIFY COLUMN first_name SET MASKING POLICY name_policy, last_name SET MASKING POLICY name_policy;",
            "D": "ALTER TABLE customer MODIFY COLUMN first_name ADD MASKING POLICY name_policy, last_name SET MASKING POLICY name_policy;"
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 64",
        "text": "A Data Engineer is working on a Snowflake deployment in AWS eu-west-1 (Ireland). The Engineer is planning to load data from staged files into target tables using the COPY INTO command.\nWhich sources are valid? (Choose three.)",
        "options": {
            "A": "Internal stage on GCP us-central1 (Iowa)",
            "B": "Internal stage on AWS eu-central-1 (Frankfurt)",
            "C": "External stage on GCP us-central1 (Iowa)",
            "D": "External stage in an Amazon S3 bucket on AWS eu-west-1 (Ireland)",
            "E": "External stage in an Amazon S3 bucket on AWS eu-central-1 (Frankfurt)",
            "F": "SSD attached to an Amazon EC2 instance on AWS eu-west-1 (Ireland)"
        },
        "correct_answers": [
            "C",
            "D",
            "E"
        ]
    },
    {
        "id": "Question 65",
        "text": "A Data Engineer wants to create a new development database (DEV) as a clone of the permanent production database (PROD). There is a requirement to disable Fail-safe for all tables.\nWhich command will meet these requirements?",
        "options": {
            "A": "CREATE DATABASE DEV - CLONE PROD - FAIL_SAFE = FALSE;",
            "B": "CREATE DATABASE DEV - CLONE PROD;",
            "C": "CREATE TRANSIENT DATABASE DEV - CLONE PROD;",
            "D": "CREATE DATABASE DEV - CLONE PROD - DATA_RETENTION_TIME_IN DAYS = 0;"
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 66",
        "text": "A Data Engineer is building a data pipeline to ingest incremental data from a data source using streams and tasks in Snowflake. A stream is created on a CUSTOMER_RAW table to track the new records and merge them into a CUSTOMER master table by way of a task that runs every hour.\nWhat will occur if the CUSTOMER_RAW table gets renamed to CUSTOMER_BASE after the task runs for three days?",
        "options": {
            "A": "The DESCRIBE STREAM for the stream will show the value of the STALE column as TRUE.",
            "B": "The renaming will cause no disruption to the existing data pipeline.",
            "C": "The DESCRIBE STREAM for the stream will show the value of the STALE column as FALSE.",
            "D": "The TASK_HISTORY for the subsequent run will show an error after the renaming operation."
        },
        "correct_answers": [
            "C"
        ]
    },
    {
        "id": "Question 67",
        "text": "Which query will show a list of the 20 most recent executions of a specified task, MYTASK, that have been scheduled within the last hour that have ended or are still running?",
        "options": {
            "A": "",
            "B": "",
            "C": "",
            "D": ""
        },
        "correct_answers": [
            "B"
        ]
    }
]